Q1.Difference between Hadoop 1 and Hadoop 2?
Ans. 
In Hadoop 1 ,there is a single NameNode which is thus the single point of failure whereas, in Hadoop 2.x,
there are Active and Passive NameNodes. In case, the active NameNode fails, the passive NameNode replaces the active NameNode and takes the charge. As a result, high availability is there 
in Hadoop 2.x.

In Hadoop 2 , the YARN provides a central resource manager that share a common resource to run multiple applications 
in Hadoop whereas data processing is a problem in Hadoop 1.x.
Hadoop 1 is supported only by Linux 
Hadoop 2 is supported by both Linux and Windows.

Q2.In hadoop 2 why the block size has been set to 128 mb?
Ans.
In Hadoop, input data files are divided into blocks of a prticular size(128 mb by default) and then these blocks of 
data are stored on different data nodes.

Hadoop is designed to process large volumes of data. Each block’s information(its address ie on which data node it 
is stored) is placed in namenode. So if the block size is too small, then there will be a large no. of blocks to be 
stored on data nodes as well as a large amount of metadata information needs to be stored on namenode, Also each 
block of data is processed by a Mapper task. If there are large no. of small blocks, a lot of mapper tasks will be 
required. So having small block size is not very efficient.

Also the block size should not be very large such that , parallelism cant be achieved. It should not be such that 
the system is waiting a very long time for one unit of data processing to finish its work.

A balance needs to be maintained. That’s why the default block size is 128 MB. It can be changed as well depending 
on the size of input files.

Q3.Why name node is relay on memory rather than datanode?
Ans.
Name Node only store metadata which is related to the different blocks and because of this reason it needs high 
memory space. Data Nodes don’t need large memory space.

Q4. Suppose you have 10 PB of data. Metadata is actually store object of file and folder ----> each obj 200 B. 
How much min Namenode RAM memory you need for your data node in a cluster to manage the metadata?
Estimate minimum Namenode RAM size for HDFS with 10 PB capacity, block size 64 MB, average metadata size for each 
block is 200 B, replication factor is 3. 
Ans.
10 PB/(64MB *3) *200B = (10 * 10^15)/(64* 10^6 * 3)*200 B = 10^10/(64*3)* 300B = 1.5625e10 B
